Jd 1
5+ Years of Data Modeling Experience
Proven track record of delivering complex Logical and Physical Data Models with expertise in delivering Physical Data Models.
Extensive experience with at least one BI Reporting tools like: Tableu/Looker/Cognos/MicroStrategy/Power BI etc.
Expertise in Ralph Kimball Style of Modeling: Dimensional, Multidimensional, Star, Galaxy and Snowflake Schema.
Extensive experience with either ER Studio or Erwin Data Modeling tool.
Extensive experience is versioning, maintaining Data Modeling Artifacts like files, repositories, Definitions, Mappings, Data Dictionary etc.
Extensive experience with Business Requirements Analysis and Data Analysis using SQL scripts, Excel techniques.
Must be passionate about being up to date with current industry trends and experimenting/learning new technologies.
Demonstrate deep knowledge od different Data Warehousing Methodologies, techniques and best practices.
Deep knowledge of at least one relational database: Oracle/SQL Server/Teradata. Performance tuning, Partitioning, Indexing of Physical Database Objects.
Excellent verbal and written communication skills to work with: Team Leads, Project Managers, ETL Team, Business Analysts, Data Analysts, DBAs etc Self-Starter with a research problem solving mindset.
Experience with cloud technologies like Snowflake, Azure, AWS, GCP etc.
Experience with Data Vault Modeling. A Bachelor’s Degree in Computer Science, Engineering, Math, or other related STEM discipline

Jd 2
Proven experience in developing logical and physical relational data models for OLTP databases.
Proficiency in using Erwin or Toad Data Modeler to create and maintain logical and physical data models.
Expertise in complex PL/SQL, T-SQL, and SQL development against SQL Server.
Ability to identify and troubleshoot data quality issues effectively.
Experience in developing and managing ELT/ETL processes.
Familiarity with Agile/Scrum development methodologies.
Competence in utilizing source control and release technologies such as Git and Azure DevOps.
A Master's Degree in a computer-related field.

Jd 3
Bachelor's or Master's degree in Computer Science, Information Systems, Finance or related field.
Experience in data modeling
Proficiency in data modeling techniques and tools, such as ER/Studio, etc.
Excellent analytical and problem-solving skills, with the ability to identify and resolve complex data modeling issues and challenges.
Good understanding of data management concepts and practices, such as data quality, metadata management, data lineage, data integration, data governance, etc.
Strong communication and collaboration skills, with the ability to work effectively with cross-functional teams and stakeholders.
Experience with data analysis and reporting tools, such as SQL, Tableau, Excel, etc.

Jd 4
Experience level must be 8 + years for this role
Drive data architecture and solution design across the platform.
Drive Data modelling, Securing, Cost management and commercialize the data.
Design and architect an Azure Data Factory, Azure Databricks, Azure Data Lake and ETLs.
Collaborate with other departments to design and build the data models needed to derive dashboards, reports and insights.
Create conceptual, logical and physical data models for both relational and dimensional solutions.
Must have work experience in Claims, Clinical and population health space.

Jd 5
Minimum of 4 years' applied experience as a Data Modeler / Data Architect in a Data-warehouse environment
Minimum of 4 years' applied experience with conceptual, logical and physical design of Atomic (3rd normal form) and Data marts (Dimensional Star schema modeling)
Minimum of 5 years applied on experience with Oracle RDBMS
Minimum of 3 years applied experience with data profiling, performance tuning of Oracle database & data marts
Minimum of 5 years' experience with SDLC / Scaled Agile delivery frameworks for data warehouse projects - inclusive of standards development.
Hands on experience in performance tuning of Oracle database, data marts
Complete understanding of Business Intelligence reporting
Preferred: Exposure to BI reporting tools like Cognos & PowerBI Experience with Data governance in a typical data-warehouse to maintain quality and accuracy of data

Jd 6
Bachelor's degree in computer science, information technology, or a similar field.
5+ years of hands-on experience with conceptual and logical data modeling.
3+ years of hands-on experience with physical and relational data modeling.
Strong technical experience with data modeling tools.
Expert knowledge of metadata management and related tools.
Strong interpersonal skills.
Excellent communication and presentation skills.
Understanding of web services and application integration.
Strong familiarity with relational databases and SQL.
Exposure to NoSQL databases and data structure design

Jd 7
Bachelor's Degree or equivalent in a technology related field (e.g. Computer Science, Engineering, etc.) with a focus on data analysis, data structures, or data modeling.
Understanding of modeling strategies - dimensional, relational, semi-unstructured (e.g. JSON, XML etc.).
Robust SQL experience.
Proven track record of solving real world problems using a design-oriented understanding of different database (e.g. Oracle, Snowflake, MongoDB) and streaming (e.g. Kafka) platforms.
Expert level analysis skills in the form of research, gap analysis, data mining, and data profiling.
Familiarity with enterprise data lakes and Cloud data platforms such as Snowflake.
Financial domain knowledge is a huge plus

Jd8
Minimum of five (5) years’ experience within the MHS Data Warehouses
High Degree of experience in data modeler designs, implements, and documents data architecture and data modeling solutions, which include the use of relational, dimensional, and NoSQL databases. These solutions support enterprise information management, business intelligence, machine learning, data science, and other business interests. Be responsible for the development of the conceptual, logical, and physical data models, the implementation of RDBMS, operational data store (ODS), data marts, and data lakes on target platforms (SQL/NoSQL).
Advanced SQL Server, Redshift and/or Cerner Electronic Health Record HealtheIntent
Five plus (5+) years of hands-on relational, dimensional, and/or analytic experience (using RDBMS, dimensional, NoSQL data platform technologies, and ETL and data ingestion protocols).
Five plus (5+) years Team lead experience to include team management, communication, and presentation.
Experience with data warehouse, data lake, and enterprise big data platforms in multi-data-center contexts required.
Advanced knowledge of metadata management, data modeling, and related tools (Erwin or ER Studio or others) required.
Data query design and implementation including automation, performance, and completeness surveillance across M2 (Business Objects), MDR (SAS), or Redshift/AWS, Cerner Discern and HealtheIntent Query author, data author experience CHCS Cache, SQL, MDR, SAS, HL7, ETL and HealtheAnalytics experience Advanced Relational Database and Modeling experience.
Experience and ability to oversee and govern the expansion of existing data architecture and the optimization of data query performance via best practices. The candidate must be able to work independently and collaboratively.
Experience and ability to implement business and IT data requirements through new data strategies and designs across all data platforms (relational, dimensional, and NoSQL) and data tools (reporting, visualization, analytics, and machine learning).
Experience and ability to work with business and application/solution teams to implement data strategies, build data flows, and develop conceptual/logical/physical data models
Experience and ability to define and govern data modeling and design standards, tools, best practices, and related development for enterprise data models.
Experience and ability to identify the architecture, infrastructure, and interfaces to data sources, tools supporting automated data loads, security concerns, analytic models, and data visualization.
Experience and ability to conduct hands-on modeling, design, configuration, installation, performance tuning, and sandbox POC.
Experience and ability to work proactively and independently to address project requirements and articulate issues/challenges to reduce project delivery risks.
Ability to acquire skills/capabilities necessary to meet growing needs/demands of systems/software/hardware.
Ability to critically examine and evaluate, problem-solve.
Ability to deliver products on time, on schedule, within budget. Flexibility and ability to adapt to rapidly changing and often time constrained environment.
Demonstrated ability to communicate analytical discoveries and appropriate recommendations/mitigation strategies effectively and clearly to all levels of customers including Senior DHA leadership.
Demonstrated ability to organize/participate/lead working groups to develop analytic products and byproducts or to develop/understand processes leading to effective optimization of analytic efforts.
Demonstrated ability to provide accurate and timely analytical products containing well-reasoned and cogent discussion points providing leadership with substantiated options or courses of action.
Demonstrated ability to undertake and complete multiple tasks with multiple deadlines simultaneously.
Demonstrated advanced proficiency in Microsoft Office products PLUS additional software/hardware skills and capabilities.
Proven ability to synthesize disparate data from multiple sources and coalesce into an accurate and useful analytic product, incorporating Service and MHS strategic goals for use by leadership in both tactical and strategic decision making.

Jd 9
Understand and translate business needs into data models supporting long-term solutions.
Work with the Application Development team to implement data strategies, build data flows and develop conceptual data models.
Create logical and physical data models using best practices to ensure high data quality and reduced redundancy.
Optimize and update logical and physical data models to support new and existing projects.
Maintain conceptual, logical and physical data models along with corresponding metadata.
Develop best practices for standard naming conventions and coding practices to ensure consistency of data models.
Recommend opportunities for reuse of data models in new environments.
Perform reverse engineering of physical data models from databases and SQL scripts.
Evaluate data models and physical databases for variances and discrepancies.
Validate business data objects for accuracy and completeness.
Analyze data-related system integration challenges and propose appropriate solutions.
Develop data models according to company standards.
Guide System Analysts, Engineers, Programmers and others on project limitations and capabilities, performance requirements and interfaces.
Review modifications to existing software to improve efficiency and performance.
Examine new application design and recommend corrections if required.

Jd 10
Bachelor’s Degree in STEM related field or equivalent
Ten years of related experience
Advanced knowledge of tools, techniques, and manipulation including cloud platforms, programming languages, and modern software engineering practices.
Strong delivery skills including the ability to determine the software design strategy and methodology to be used for efforts, use automated tests, analysis, and informed feedback loops to ensure the quality and production readiness of work before release, monitor the health of work efforts and that of adjacent systems.
Demonstrated track record of domain expertise including the ability to develop business partnerships and influence priorities by identifying solutions that are aligned with current business objective and closely follow industry trends relevant to domain, understanding how to apply them, and sharing knowledge with coworkers.
Strong problem solver who utilizes data and proofs of concepts to find creative solutions to difficult problems involving a significant number of factors with broad implications, reflects on solutions, measures impact, and uses that information to ideate and optimize.
Excellent communication skills with the ability to develop business partnerships, describe technology concepts in ways the business can understand, document initiatives in a concise and clear manner, and empathetically and attentively listen to others thoughts and ideas.
Ability to lead and take action even when there is no clear owner, inspire and motivate others, and be effective at influencing team members.

Jd 11
Min. year 5-6 years of hands-on data modelling experience
Broad expertise in data technologies, i.e., OLTP and OLAP data modeling, data warehousing, ETL, data quality concepts, Business Intelligence tools and analytical tools, unstructured data, machine learning, SQL and No-SQL databases, etc
Excellent Command of SQL

Jd 12
 Excellent command of the SQL language
• Broad expertise in data technologies; i.e., OLTP and OLAP data modeling, data warehousing, ETL, data quality concepts, Business Intelligence tools and analytical tools, unstructured data, machine learning, SQL and No-SQL databases, etc.
• Familiarity with Event Driven architecture and Microservices (nice to have)
• Technical understanding of common relational database systems; i.e., Teradata and Oracle
• Understanding of Hadoop-related technologies & their applications
• Advanced analytical thinking and problem-solving skills
• Hands-on experience with various data modeling techniques and tools, ERWIN is a plus
• Able to manage influence upward and downward within function/department

Jd 13
Bachelor's degree in Information Technology.
Experience with Entity Relationship Diagrams and data modeling tools - Erwin knowledge a plus.
Excellent technical, analytical / problem solving and organizational skills.
Strong collaboration, communication and interpersonal skills.
Experience with DB2 LUW, Cassandra, DataStax and SQL Server to query data.

Jd 14
Bachelor's Degree in related technical field of study.
12+ year's data modeling experience with large scale models.
10+ years' experience with data modeling tools (Erwin / ER Studio).
10+ years' experience in Data Architecture and Database design patterns.
10 + years' experience mapping data elements for ETL purposes.
10 + years' experience generating physical DDL to deploy to databases Ability to reverse engineer existing databases.
Ability to build both transactional as well as dimensional models.
Thorough understanding of database technologies (UNIX, DB2, SQL, Data Quality).

Jd 15
Experience 10+ Years of relevant experience.
Need ETL design strong P&C background.
Must have strong experience in Data Analysis in an enterprise DWH environment.
Advanced knowledge in SQL writing is required.
Strong analyst background in data-oriented applications.
Must have experience working in large, fast-paced Agile projects.
Knowledge of the Mortgage Insurance Industry must be able to understand P&C data.

Jd 16
Primary skill: SQL Expertise, Banking Data Modeler, expertise in Data Vault modeling in Banking domain, Financial Services experience in the Regulatory Data domain
Secondary skill: Data warehouse concepts, data modelling tool experience
Qualifications:
Bachelor's or Master’s degree in Computer Science, Information Systems, or a related field.
Proven experience as a Banking Data Modeler within regulatory compliance, credit risk, and finance, specifically Financial Services experience in the Regulatory Data domain, showcasing expertise in Data Vault modeling and other relevant methodologies.
Proficient in data modeling tools.
Strong SQL skills and deep understanding of relational database management systems.
Comprehensive knowledge of data warehousing concepts and Financial Services experience in the Regulatory Data domain.
Excellent analytical and problem-solving abilities, particularly in addressing data challenges unique to regulatory compliance, credit risk, and finance domains.
Effective communication skills, with the ability to convey complex concepts to both technical and non-technical stakeholders.
Detail-oriented and committed to maintaining the highest standards of data quality regulatory compliance and industry standards.

Jd 17
Skillset Required Advanced proficiency in data modeling tools and methodologies specifically geared towards the design of complex data products including both derived and prime data products
Deep understanding of data warehousing and big data technologies with the ability to model data in both traditional and modern scalable environments
Strong knowledge in data processing transformation and extraction techniques
Familiarity with data governance data quality principles and metadata management

Jd 18
4 to 6 years of proven experience as a lead data modeler, preferably in the banking or financial industry
Deep knowledge and hands on experience with Snowflake and Oracle environments
Experience using SQLdbm and dbt to model and create physical data models
Experience documenting data model definitions to support lineage and data governance efforts
Detail-oriented mindset with the ability to prioritize and manage multiple tasks in a dynamic environment.
Strong problem-solving skills and the ability to think critically to identify and resolve business challenges.
Bachelor's degree in Business Administration, Computer Science, or a related field.
Excellent communication skills with the ability to effectively collaborate with stakeholders at various levels  

Jd 19
3 + yrs in using Erwin Tool
Should develop Physical model design using Erwin
Generate Reports with Erwin
Perform Profiling
Perform Model Validation
Make changes to existing design as per Blue print provided and ensure performance and data flow is right.
Hands on experience in Teradata.
Knowledge in PL/SQL

Jd 20
Bachelor's Degree in computer science, information technology or a similar field, or equivalent work experience
3 Years of hands on data modeling
Experience with metadata management and related tools
Experience with relational, dimensional, and NoSQL data modeling
Experience with data management standards and data model practices
Experience with physical data structures in MSSQL, Oracle and Snowflake using Erwin
Experience with SDLC and DevOps
